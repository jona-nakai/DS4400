{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso as SKLasso, Ridge as SKRidge, ElasticNet as SKElastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array([4, 7, 9, 11, 15])\n",
    "b1 = np.array([18, 22, 25, 30, 35, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.2"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(a1, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.26666666666668"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(b1, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = np.array([12, 14, 16, 18, 20])\n",
    "b2 = np.array([28, 32, 36, 40, 44, 48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(a2, ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.666666666666664"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(b2, ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-1, 0, 1, 1]).reshape(-1, 1)\n",
    "X = np.array([[1, -2, 0, 1],\n",
    "              [-2, -1, 1, 2],\n",
    "              [1, 2, -1, 1]])\n",
    "Y = np.array([2, 3, -1]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_f(w):\n",
    "    return 2*X.T @ (X@w - Y) + 2*2*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.],\n",
       "       [-2.],\n",
       "       [-3.],\n",
       "       [-3.]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = w0 - 0.5 * d_f(w0)\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-65.],\n",
       "       [-18.],\n",
       "       [ 31.],\n",
       "       [ 41.]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = w1 - 0.5 * d_f(w1)\n",
    "w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 10)\n",
      "(300, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.genfromtxt('stock_prediction_data.csv', delimiter=',')\n",
    "y = np.genfromtxt('stock_price.csv', delimiter=',').reshape(-1, 1)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = PolynomialFeatures(degree=2, include_bias=True).fit_transform(X_train)\n",
    "X_val = PolynomialFeatures(degree=2, include_bias=True).fit_transform(X_val)\n",
    "X_test = PolynomialFeatures(degree=2, include_bias=True).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y):\n",
    "    return np.mean((y_pred - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_d_f(X, w, y, λ):\n",
    "    return 2/(X.shape[0]) * X.T @ (X @ w - y) + 2*λ*np.sign(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ=0 Validation MSE: 0.09294580769390001\n",
      "λ=0.25 Validation MSE: 0.6481430052279473\n",
      "λ=0.5 Validation MSE: 2.3135561251651287\n",
      "λ=0.75 Validation MSE: 5.014351200194995\n",
      "λ=1 Validation MSE: 7.729218232939105\n",
      "λ=1.5 Validation MSE: 12.82838021897792\n",
      "λ=2 Validation MSE: 20.3913536839422\n",
      "λ=3 Validation MSE: 36.902216912470756\n",
      "\n",
      "Best lambda value: 0\n",
      "λ=0 Test MSE: 0.09294580769390001\n"
     ]
    }
   ],
   "source": [
    "λ_list = [0, 0.25, 0.5, 0.75, 1, 1.5, 2, 3]\n",
    "best_w = None\n",
    "best_λ = None\n",
    "best_MSE = None\n",
    "\n",
    "for λ in λ_list:\n",
    "    w = np.ones(X_train.shape[1]).reshape(-1, 1)\n",
    "\n",
    "    for i in range(10000):\n",
    "        w = w - 0.01 * lasso_d_f(X_train, w, y_train, λ)\n",
    "    \n",
    "    print(\"λ=\" + str(λ) + \" Validation MSE:\", mse(X_val @ w, y_val))\n",
    "\n",
    "    if best_MSE == None:\n",
    "        best_w = w\n",
    "        best_λ = λ\n",
    "        best_MSE = mse(X_val @ w, y_val)\n",
    "    elif mse(X_val @ w, y_val) < best_MSE:\n",
    "        best_w = w\n",
    "        best_λ = λ\n",
    "        best_MSE = mse(X_val @ w, y_val)\n",
    "\n",
    "print()\n",
    "print(\"Best lambda value:\", best_λ)\n",
    "print(\"λ=\"+str(best_λ)+\" Test MSE:\", best_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ=0 Validation MSE: 0.09294325090399339\n",
      "λ=0.25 Validation MSE: 0.5848921507490163\n",
      "λ=0.5 Validation MSE: 2.1051106412283676\n",
      "λ=0.75 Validation MSE: 4.617968284978606\n",
      "λ=1 Validation MSE: 7.629002980184824\n",
      "λ=1.5 Validation MSE: 13.05233856483137\n",
      "λ=2 Validation MSE: 20.535723261563653\n",
      "λ=3 Validation MSE: 38.36305551717119\n",
      "\n",
      "Best lambda value: 0\n",
      "λ=0 Test MSE: 0.09294325090399339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonanakai/anaconda3/envs/ds/lib/python3.12/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/jonanakai/anaconda3/envs/ds/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/jonanakai/anaconda3/envs/ds/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.800e+00, tolerance: 1.255e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "λ_list = [0, 0.25, 0.5, 0.75, 1, 1.5, 2, 3]\n",
    "best_w = None\n",
    "best_λ = None\n",
    "best_MSE = None\n",
    "\n",
    "for λ in λ_list:\n",
    "    sk_poly_lasso = SKLasso(alpha=λ)\n",
    "    sk_poly_lasso.fit(X_train,y_train.flatten()) # y is 2D, but scikit-learn expects 1D\n",
    "    pred_val = sk_poly_lasso.predict(X_val).reshape(-1,1)\n",
    "    \n",
    "    print(\"λ=\" + str(λ) + \" Validation MSE:\", mse(y_val, pred_val))\n",
    "\n",
    "    if best_MSE == None:\n",
    "        best_w = w\n",
    "        best_λ = λ\n",
    "        best_MSE = mse(y_val, pred_val)\n",
    "    elif mse(y_val, pred_val) < best_MSE:\n",
    "        best_w = w\n",
    "        best_λ = λ\n",
    "        best_MSE = mse(y_val, pred_val)\n",
    "\n",
    "print()\n",
    "print(\"Best lambda value:\", best_λ)\n",
    "print(\"λ=\"+str(best_λ)+\" Test MSE:\", best_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE for my Lasso Constraint Gradient Descent code, approximately 0.093, was almost the same as the MSE for the Sklearn Lasso Constraint, approximately 0.093."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_d_f(X, w, y, λ):\n",
    "    return 2*X.T @ (X@w - y) + 2*λ*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ=0 Validation MSE: 0.0929432509065546\n",
      "λ=0.25 Validation MSE: 0.09632926686199482\n",
      "λ=0.5 Validation MSE: 0.09996984059458619\n",
      "λ=0.75 Validation MSE: 0.10385804104081169\n",
      "λ=1 Validation MSE: 0.10798747846137201\n",
      "λ=1.5 Validation MSE: 0.11694681755202548\n",
      "λ=2 Validation MSE: 0.1268052376398113\n",
      "λ=3 Validation MSE: 0.14907389969957377\n",
      "\n",
      "Best lambda value: 0\n",
      "λ=0 Test MSE: 0.0929432509065546\n"
     ]
    }
   ],
   "source": [
    "λ_list = [0, 0.25, 0.5, 0.75, 1, 1.5, 2, 3]\n",
    "best_w = None\n",
    "best_λ = None\n",
    "best_MSE = None\n",
    "\n",
    "for λ in λ_list:\n",
    "    w = np.ones(X_train.shape[1]).reshape(-1, 1)\n",
    "\n",
    "    for i in range(10000):\n",
    "        w = w - 0.0001 * ridge_d_f(X_train, w, y_train, λ)\n",
    "    \n",
    "    print(\"λ=\" + str(λ) + \" Validation MSE:\", mse(X_val @ w, y_val))\n",
    "\n",
    "    if best_MSE == None:\n",
    "        best_w = w\n",
    "        best_λ = λ\n",
    "        best_MSE = mse(X_val @ w, y_val)\n",
    "    elif mse(X_val @ w, y_val) < best_MSE:\n",
    "        best_w = w\n",
    "        best_λ = λ\n",
    "        best_MSE = mse(X_val @ w, y_val)\n",
    "\n",
    "print()\n",
    "print(\"Best lambda value:\", best_λ)\n",
    "print(\"λ=\"+str(best_λ)+\" Test MSE:\", best_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ=0 Validation MSE: 0.09149828125000004\n",
      "λ=0.25 Validation MSE: 0.09531032106495477\n",
      "λ=0.5 Validation MSE: 0.09792788380075418\n",
      "λ=0.75 Validation MSE: 0.10079229994113074\n",
      "λ=1 Validation MSE: 0.10389999417330338\n",
      "λ=1.5 Validation MSE: 0.11083122633218859\n",
      "λ=2 Validation MSE: 0.11869420111033205\n",
      "λ=3 Validation MSE: 0.13711048894364736\n",
      "\n",
      "Best lambda value: 0\n",
      "λ=0 Test MSE: 0.09149828125000004\n"
     ]
    }
   ],
   "source": [
    "λ_list = [0, 0.25, 0.5, 0.75, 1, 1.5, 2, 3]\n",
    "best_w = None\n",
    "best_λ = None\n",
    "best_MSE = None\n",
    "\n",
    "for λ in λ_list:\n",
    "    sk_poly_ridge = SKRidge(alpha=λ)\n",
    "    sk_poly_ridge.fit(X_train,y_train.flatten()) # y is 2D, but scikit-learn expects 1D\n",
    "    pred_val = sk_poly_ridge.predict(X_val).reshape(-1,1)\n",
    "    \n",
    "    print(\"λ=\" + str(λ) + \" Validation MSE:\", mse(y_val, pred_val))\n",
    "\n",
    "    if best_MSE == None:\n",
    "        best_w = w\n",
    "        best_λ = λ\n",
    "        best_MSE = mse(y_val, pred_val)\n",
    "    elif mse(y_val, pred_val) < best_MSE:\n",
    "        best_w = w\n",
    "        best_λ = λ\n",
    "        best_MSE = mse(y_val, pred_val)\n",
    "\n",
    "print()\n",
    "print(\"Best lambda value:\", best_λ)\n",
    "print(\"λ=\"+str(best_λ)+\" Test MSE:\", best_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE for my Ridge Constraint Gradient Descent code, approximately 0.093, was almost the same as the MSE for the Sklearn Ridge Constraint, approximately 0.091."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_d_f(X, w, y, λ):\n",
    "    return 2*X.T @ (X@w - y) + 2*λ*np.sign(w) + 2*λ*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ=0 Validation MSE: 0.0929432509065546\n",
      "λ=0.25 Validation MSE: 0.09266365424469683\n",
      "λ=0.5 Validation MSE: 0.0924768241225078\n",
      "λ=0.75 Validation MSE: 0.09240428564745783\n",
      "λ=1 Validation MSE: 0.09347114200230527\n",
      "λ=1.5 Validation MSE: 0.09583804791164616\n",
      "λ=2 Validation MSE: 0.09825181950067377\n",
      "λ=3 Validation MSE: 0.1067080433573084\n",
      "\n",
      "Best lambda value: 0.75\n",
      "λ=0.75 Test MSE: 0.09240428564745783\n"
     ]
    }
   ],
   "source": [
    "λ_list = [0, 0.25, 0.5, 0.75, 1, 1.5, 2, 3]\n",
    "best_w = None\n",
    "best_λ = None\n",
    "best_MSE = None\n",
    "\n",
    "for λ in λ_list:\n",
    "    w = np.ones(X_train.shape[1]).reshape(-1, 1)\n",
    "\n",
    "    for i in range(10000):\n",
    "        w = w - 0.0001 * elastic_d_f(X_train, w, y_train, λ)\n",
    "    \n",
    "    print(\"λ=\" + str(λ) + \" Validation MSE:\", mse(X_val @ w, y_val))\n",
    "\n",
    "    if best_MSE == None:\n",
    "        best_w = w\n",
    "        best_λ = λ\n",
    "        best_MSE = mse(X_val @ w, y_val)\n",
    "    elif mse(X_val @ w, y_val) < best_MSE:\n",
    "        best_w = w\n",
    "        best_λ = λ\n",
    "        best_MSE = mse(X_val @ w, y_val)\n",
    "\n",
    "print()\n",
    "print(\"Best lambda value:\", best_λ)\n",
    "print(\"λ=\"+str(best_λ)+\" Test MSE:\", best_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ=0 Validation MSE: 0.09294325090399339\n",
      "λ=0.25 Validation MSE: 1.3093410881089398\n",
      "λ=0.5 Validation MSE: 3.995272505431218\n",
      "λ=0.75 Validation MSE: 7.311109090036272\n",
      "λ=1 Validation MSE: 10.827746499666107\n",
      "λ=1.5 Validation MSE: 17.722082297295813\n",
      "λ=2 Validation MSE: 23.9603694212866\n",
      "λ=3 Validation MSE: 32.984971725551866\n",
      "\n",
      "Best lambda value: 0\n",
      "λ=0 Test MSE: 0.09294325090399339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonanakai/anaconda3/envs/ds/lib/python3.12/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Users/jonanakai/anaconda3/envs/ds/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/jonanakai/anaconda3/envs/ds/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.800e+00, tolerance: 1.255e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "λ_list = [0, 0.25, 0.5, 0.75, 1, 1.5, 2, 3]\n",
    "best_w = None\n",
    "best_λ = None\n",
    "best_MSE = None\n",
    "\n",
    "for λ in λ_list:\n",
    "    sk_poly_elastic = SKElastic(alpha=λ)\n",
    "    sk_poly_elastic.fit(X_train,y_train.flatten()) # y is 2D, but scikit-learn expects 1D\n",
    "    pred_val = sk_poly_elastic.predict(X_val).reshape(-1,1)\n",
    "    \n",
    "    print(\"λ=\" + str(λ) + \" Validation MSE:\", mse(y_val, pred_val))\n",
    "\n",
    "    if best_MSE == None:\n",
    "        best_w = w\n",
    "        best_λ = λ\n",
    "        best_MSE = mse(y_val, pred_val)\n",
    "    elif mse(y_val, pred_val) < best_MSE:\n",
    "        best_w = w\n",
    "        best_λ = λ\n",
    "        best_MSE = mse(y_val, pred_val)\n",
    "\n",
    "print()\n",
    "print(\"Best lambda value:\", best_λ)\n",
    "print(\"λ=\"+str(best_λ)+\" Test MSE:\", best_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE for my Elastic Net Gradient Descent code, approximately 0.093, was almost the same as the MSE for the Sklearn Elastic Net, approximately 0.093."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best lmabda value for each constraint was 0, making the contraint function irrelevant. If we compare the constraint functions for nonzero lambda values, for example 1, we get Lasso: 7.63, Ridge: 0.10, and Elastic: 10.83. So Elasitic Net appears to perform worse than using only one constraint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
